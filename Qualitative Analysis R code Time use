library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(readstata13)  #for reading the stata files
library(dplyr)
library(haven)##foreign 
library(htmlTable)##html tables
library(magrittr)  #manipulate
library(loose.rock) 
library(tidyverse)

df1<- read.dta13("C:/Users/user/Desktop/TimeuseData/fdata/tus_activities.dta")
names(df1)
a<-head(df1)
View(a)
#analyzing qualitative data for time use using R. Write the data to .txt
kh<-df1$t03
write.table(kh, append = FALSE, quote = TRUE,sep = " ",
            file="C:/Users/user/Desktop/qualitative/md.txt", 
            row.names = FALSE, col.names = FALSE)


text <- readLines("C:/Users/user/Desktop/qualitative/md.txt")
docs <- Corpus(VectorSource(text))
#docs
inspect(docs)

toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")

# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("I", "my","taking","took","preparing",
                                    "sleeping","slept","lunch","supper",
                                    "breakfast","eating","working","work",
                                    "home","watching","farm","cleaning",
                                    "washing","relaxing","sleep","night")) 
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
docs_matrix <- TermDocumentMatrix(docs)
m <- as.matrix(docs_matrix)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d,20)

wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col ="lightblue", main ="Most commonly used words......",
        ylab = "Word frequencies", xlab="Keywords")



####srt detect
k<-data.frame(df1$supervisor_name, df1$interviewer_name,df1$a01, df1$a08, df1$t03,df1$t08, df1$t11)
head(k)
#k
k %>% 
  filter((str_detect(df1.t03, "child"))) %>% # | str_detect(df1.t03, "breakfast")| str_detect(df1.t03, "supper"))%>%
  group_by(df1.t11)%>% summarise(n=n())  %>% select(df1.t11,n) %>% htmlTable

###filtering employment in goverment and coded sleeping in T03
levels(k$df1.t11) #find levels of main activity minding the spaces
k1<-k %>% filter(df1.t11=="311 Preparing meals/snacks")
#k1
#k2<-k1 %>% 
#  filter((str_detect(df1.t03, "lunch")) | str_detect(df1.t03, "supper")| str_detect(df1.t03, "breakfast"))
#d<-k2

k2<-k1 %>% 
  filter((str_detect(df1.t03, "child")))
d<-k2
d %>% htmlTable


results_clipboard <- function(d, sep="\t", dec=".", max.size=(200*1000))   # Copy a data.frame to clipboard
{
  write.table(d, paste0("clipboard-", formatC(max.size, format="f", digits=0)), sep=sep, row.names=TRUE, dec=dec)
}
results_clipboard(d) ###then paste in excel


